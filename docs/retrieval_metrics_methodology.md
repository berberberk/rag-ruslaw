# Методология оценки retrieval (Phase 6.5)

## Цель оценки
- Измеряем способность retriever'ов возвращать релевантный контекст для вопросов evalset.
- Критично для RAG: качественный контекст напрямую влияет на корректность ответа и цитирование источников.

## Данные и разметка
- Evalset в `data/eval/evalset.jsonl`: записи с полями `id`, `question`, `gold_doc_ids`, `gold_chunk_ids`, `type` (definition/fact/negative), `notes`, `source`.
- Reference задаётся вручную/полуавтоматически: `gold_doc_ids` (обязательно для non-negative), `gold_chunk_ids` (если автопривязка удалась).
- Negative вопросы: `gold_doc_ids=[]`, `gold_chunk_ids=[]` — ожидается отсутствие релевантных документов.

## Допущения
- Reference context задан вручную/автопривязкой, но неполный; допускается шум.
- Релевантность трактуется бинарно относительно reference (нет градаций).
- Вопросы охватывают ограниченный срез RusLawOD; возможен bias по типам документов.

## Метрики и определения
- **Retrieval-only:**
  - `hit@k` (chunk): 1, если в top-k есть хотя бы один `chunk_id` из `gold_chunk_ids`, иначе 0.
  - `doc_hit@k`: 1, если в top-k есть хотя бы один `doc_id` из `gold_doc_ids`, иначе 0.
  - `precision@k` (chunk): доля найденных релевантных чанков среди top-k (None, если `gold_chunk_ids` пуст).
  - `recall@k` (chunk): доля покрытых reference чанков в top-k (None, если `gold_chunk_ids` пуст). Если `gold_chunk_ids` неизвестны, используем `doc_hit@k` как прокси.
- **RAGAS (семантические):**
  - `context_precision`: насколько возвращённый контекст релевантен вопросу.
  - `context_recall`: насколько покрыт референсный контекст вопроса.
  - Использование RAGAS опционально; при отсутствии LLM — расчёт только retrieval-only метрик.

## Работа при неполной разметке
- Если `gold_chunk_ids` пусты, но есть `gold_doc_ids`: считаем `doc_hit@k`, `hit@k`=0, precision/recall=None (фиксируем в отчёте).
- Если retrieval возвращает документы вне `gold_doc_ids` с похожими терминами: интерпретируем как потенциальный FP, но помечаем ограничение разметки.
- Negative: ожидаем `hit@k=0`, `doc_hit@k=0`; любое попадание трактуется как ошибка retriever'а или шум разметки.

## Ограничения интерпретации
- Метрики не гарантируют правильность финального ответа, только качество контекста.
- Зависимость от чанкинга и очистки текста; изменения параметров chunk_size/overlap влияют на метрики.
- Разметка ограничена выбранным срезом, возможен bias по типам документов и вопросам.
- Значения зависят от k; сравнивать retrievers нужно при фиксированных k (например, 5/10/20).
- Возможен пропуск релевантных документов из-за неполной разметки.

## Рекомендации по эксперименту и визуализации
- Считать метрики при k = 5, 10, 20; строить таблицу сравнения retrievers (bm25/dense/hybrid).
- Графики: bar chart по hit@k/doc_hit@k; boxplot по context_precision/context_recall (если считаются); линия зависимости hit@k от k.
- Фиксировать параметры чанкинга (chunk_size_chars/overlap_chars) в отчёте.

## Связь с Phase 7–8
- Метрики используются в Phase 7 (прогон RAGAS/retrieval) и Phase 8 (визуализации/графики).
- `evalset.jsonl` + скрипты автолинка/каталога обеспечивают воспроизводимость; результаты метрик и графики включаются в отчёт для сравнения bm25/dense/hybrid.
